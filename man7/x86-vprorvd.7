'\" t
.nh
.TH "X86-VPRORD-VPRORVD-VPRORQ-VPRORVQ" "7" "December 2023" "Intel" "Intel x86-64 ISA Manual"
.SH NAME
VPRORD-VPRORVD-VPRORQ-VPRORVQ - BIT ROTATE RIGHT
.TS
allbox;
l l l l l 
l l l l l .
\fBOpcode/Instruction\fP	\fBOp / En\fP	\fB64/32 bit Mode Support\fP	\fBCPUID Feature Flag\fP	\fBDescription\fP
T{
EVEX.128.66.0F38.W0 14 /r VPRORVD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst
T}	B	V/V	AVX512VL AVX512F	T{
Rotate doublewords in xmm2 right by count in the corresponding element of xmm3/m128/m32bcst, store result using writemask k1.
T}
T{
EVEX.128.66.0F.W0 72 /0 ib VPRORD xmm1 {k1}{z}, xmm2/m128/m32bcst, imm8
T}	A	V/V	AVX512VL AVX512F	T{
Rotate doublewords in xmm2/m128/m32bcst right by imm8, store result using writemask k1.
T}
T{
EVEX.128.66.0F38.W1 14 /r VPRORVQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst
T}	B	V/V	AVX512VL AVX512F	T{
Rotate quadwords in xmm2 right by count in the corresponding element of xmm3/m128/m64bcst, store result using writemask k1.
T}
T{
EVEX.128.66.0F.W1 72 /0 ib VPRORQ xmm1 {k1}{z}, xmm2/m128/m64bcst, imm8
T}	A	V/V	AVX512VL AVX512F	T{
Rotate quadwords in xmm2/m128/m64bcst right by imm8, store result using writemask k1.
T}
T{
EVEX.256.66.0F38.W0 14 /r VPRORVD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst
T}	B	V/V	AVX512VL AVX512F	T{
Rotate doublewords in ymm2 right by count in the corresponding element of ymm3/m256/m32bcst, store using result writemask k1.
T}
T{
EVEX.256.66.0F.W0 72 /0 ib VPRORD ymm1 {k1}{z}, ymm2/m256/m32bcst, imm8
T}	A	V/V	AVX512VL AVX512F	T{
Rotate doublewords in ymm2/m256/m32bcst right by imm8, store result using writemask k1.
T}
T{
EVEX.256.66.0F38.W1 14 /r VPRORVQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst
T}	B	V/V	AVX512VL AVX512F	T{
Rotate quadwords in ymm2 right by count in the corresponding element of ymm3/m256/m64bcst, store result using writemask k1.
T}
T{
EVEX.256.66.0F.W1 72 /0 ib VPRORQ ymm1 {k1}{z}, ymm2/m256/m64bcst, imm8
T}	A	V/V	AVX512VL AVX512F	T{
Rotate quadwords in ymm2/m256/m64bcst right by imm8, store result using writemask k1.
T}
T{
EVEX.512.66.0F38.W0 14 /r VPRORVD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst
T}	B	V/V	AVX512F	T{
Rotate doublewords in zmm2 right by count in the corresponding element of zmm3/m512/m32bcst, store result using writemask k1.
T}
T{
EVEX.512.66.0F.W0 72 /0 ib VPRORD zmm1 {k1}{z}, zmm2/m512/m32bcst, imm8
T}	A	V/V	AVX512F	T{
Rotate doublewords in zmm2/m512/m32bcst right by imm8, store result using writemask k1.
T}
T{
EVEX.512.66.0F38.W1 14 /r VPRORVQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst
T}	B	V/V	AVX512F	T{
Rotate quadwords in zmm2 right by count in the corresponding element of zmm3/m512/m64bcst, store result using writemask k1.
T}
T{
EVEX.512.66.0F.W1 72 /0 ib VPRORQ zmm1 {k1}{z}, zmm2/m512/m64bcst, imm8
T}	A	V/V	AVX512F	T{
Rotate quadwords in zmm2/m512/m64bcst right by imm8, store result using writemask k1.
T}
.TE

.SH INSTRUCTION OPERAND ENCODING <a
href="./vprord:vprorvd:vprorq:vprorvq.html#instruction-operand-encoding"
class="anchor">¶

.TS
allbox;
l l l l l l 
l l l l l l .
\fBOp/En\fP	\fBTuple Type\fP	\fBOperand 1\fP	\fBOperand 2\fP	\fBOperand 3\fP	\fBOperand 4\fP
A	Full	VEX.vvvv (w)	ModRM:r/m (R)	imm8	N/A
B	Full	ModRM:reg (w)	EVEX.vvvv (r)	ModRM:r/m (r)	N/A
.TE

.SS DESCRIPTION  href="./vprord:vprorvd:vprorq:vprorvq.html#description"
class="anchor">¶

.PP
Rotates the bits in the individual data elements (doublewords, or
quadword) in the first source operand to the right by the number of bits
specified in the count operand. If the value specified by the count
operand is greater than 31 (for doublewords), or 63 (for a quadword),
then the count operand modulo the data size (32 or 64) is used.

.PP
EVEX.128 encoded version: The destination operand is a XMM register. The
source operand is a XMM register or a memory location (for immediate
form). The count operand can come either from an XMM register or a
memory location or an 8-bit immediate. Bits (MAXVL-1:128) of the
corresponding ZMM register are zeroed.

.PP
EVEX.256 encoded version: The destination operand is a YMM register. The
source operand is a YMM register or a memory location (for immediate
form). The count operand can come either from an XMM register or a
memory location or an 8-bit immediate. Bits (MAXVL-1:256) of the
corresponding ZMM register are zeroed.

.PP
EVEX.512 encoded version: The destination operand is a ZMM register
updated according to the writemask. For the count operand in immediate
form, the source operand can be a ZMM register, a 512-bit memory
location or a 512-bit vector broadcasted from a 32/64-bit memory
location, the count operand is an 8-bit immediate. For the count operand
in variable form, the first source operand (the second operand) is a ZMM
register and the counter operand (the third operand) is a ZMM register,
a 512-bit memory location or a 512-bit vector broadcasted from a
32/64-bit memory location.

.SS OPERATION  href="./vprord:vprorvd:vprorq:vprorvq.html#operation"
class="anchor">¶

.EX
RIGHT_ROTATE_DWORDS(SRC, COUNT_SRC)
COUNT := COUNT_SRC modulo 32;
DEST[31:0] := (SRC >> COUNT) | (SRC << (32 - COUNT));
RIGHT_ROTATE_QWORDS(SRC, COUNT_SRC)
COUNT := COUNT_SRC modulo 64;
DEST[63:0] := (SRC >> COUNT) | (SRC << (64 - COUNT));
.EE

.SS VPRORD (EVEX ENCODED VERSIONS) <a
href="./vprord:vprorvd:vprorq:vprorvq.html#vprord--evex-encoded-versions-"
class="anchor">¶

.EX
(KL, VL) = (4, 128), (8, 256), (16, 512)
FOR j := 0 TO KL-1
    i := j * 32
    IF k1[j] OR *no writemask* THEN
            IF (EVEX.b = 1) AND (SRC1 *is memory*)
                THEN DEST[i+31:i] := RIGHT_ROTATE_DWORDS( SRC1[31:0], imm8)
                ELSE DEST[i+31:i] := RIGHT_ROTATE_DWORDS(SRC1[i+31:i], imm8)
            FI;
        ELSE
            IF *merging-masking* ; merging-masking
                THEN *DEST[i+31:i] remains unchanged*
                ELSE *zeroing-masking*
                        ; zeroing-masking
                    DEST[i+31:i] := 0
            FI
    FI;
ENDFOR
DEST[MAXVL-1:VL] := 0
.EE

.SS VPRORVD (EVEX ENCODED VERSIONS) <a
href="./vprord:vprorvd:vprorq:vprorvq.html#vprorvd--evex-encoded-versions-"
class="anchor">¶

.EX
(KL, VL) = (4, 128), (8, 256), (16, 512)
FOR j := 0 TO KL-1
    i := j * 32
    IF k1[j] OR *no writemask* THEN
            IF (EVEX.b = 1) AND (SRC2 *is memory*)
                THEN DEST[i+31:i] := RIGHT_ROTATE_DWORDS(SRC1[i+31:i], SRC2[31:0])
                ELSE DEST[i+31:i] := RIGHT_ROTATE_DWORDS(SRC1[i+31:i], SRC2[i+31:i])
            FI;
        ELSE
            IF *merging-masking* ; merging-masking
                THEN *DEST[i+31:i] remains unchanged*
                ELSE *zeroing-masking*
                        ; zeroing-masking
                    DEST[i+31:i] := 0
            FI
    FI;
ENDFOR
DEST[MAXVL-1:VL] := 0
.EE

.SS VPRORQ (EVEX ENCODED VERSIONS) <a
href="./vprord:vprorvd:vprorq:vprorvq.html#vprorq--evex-encoded-versions-"
class="anchor">¶

.EX
(KL, VL) = (2, 128), (4, 256), (8, 512)
FOR j := 0 TO KL-1
    i := j * 64
    IF k1[j] OR *no writemask* THEN
            IF (EVEX.b = 1) AND (SRC1 *is memory*)
                THEN DEST[i+63:i] := RIGHT_ROTATE_QWORDS(SRC1[63:0], imm8)
                ELSE DEST[i+63:i] := RIGHT_ROTATE_QWORDS(SRC1[i+63:i], imm8])
            FI;
        ELSE
            IF *merging-masking* ; merging-masking
                THEN *DEST[i+63:i] remains unchanged*
                ELSE *zeroing-masking*
                        ; zeroing-masking
                    DEST[i+63:i] := 0
            FI
    FI;
ENDFOR
DEST[MAXVL-1:VL] := 0
.EE

.SS VPRORVQ (EVEX ENCODED VERSIONS) <a
href="./vprord:vprorvd:vprorq:vprorvq.html#vprorvq--evex-encoded-versions-"
class="anchor">¶

.EX
(KL, VL) = (2, 128), (4, 256), (8, 512)
FOR j := 0 TO KL-1
    i := j * 64
    IF k1[j] OR *no writemask* THEN
            IF (EVEX.b = 1) AND (SRC2 *is memory*)
                THEN DEST[i+63:i] := RIGHT_ROTATE_QWORDS(SRC1[i+63:i], SRC2[63:0])
                ELSE DEST[i+63:i] := RIGHT_ROTATE_QWORDS(SRC1[i+63:i], SRC2[i+63:i])
            FI;
        ELSE
            IF *merging-masking* ; merging-masking
                THEN *DEST[i+63:i] remains unchanged*
                ELSE *zeroing-masking*
                        ; zeroing-masking
                    DEST[i+63:i] := 0
            FI
    FI;
ENDFOR
DEST[MAXVL-1:VL] := 0
.EE

.SS INTEL C/C++ COMPILER INTRINSIC EQUIVALENT <a
href="./vprord:vprorvd:vprorq:vprorvq.html#intel-c-c++-compiler-intrinsic-equivalent"
class="anchor">¶

.EX
VPRORD __m512i _mm512_ror_epi32(__m512i a, int imm);

VPRORD __m512i _mm512_mask_ror_epi32(__m512i a, __mmask16 k, __m512i b, int imm);

VPRORD __m512i _mm512_maskz_ror_epi32( __mmask16 k, __m512i a, int imm);

VPRORD __m256i _mm256_ror_epi32(__m256i a, int imm);

VPRORD __m256i _mm256_mask_ror_epi32(__m256i a, __mmask8 k, __m256i b, int imm);

VPRORD __m256i _mm256_maskz_ror_epi32( __mmask8 k, __m256i a, int imm);

VPRORD __m128i _mm_ror_epi32(__m128i a, int imm);

VPRORD __m128i _mm_mask_ror_epi32(__m128i a, __mmask8 k, __m128i b, int imm);

VPRORD __m128i _mm_maskz_ror_epi32( __mmask8 k, __m128i a, int imm);

VPRORQ __m512i _mm512_ror_epi64(__m512i a, int imm);

VPRORQ __m512i _mm512_mask_ror_epi64(__m512i a, __mmask8 k, __m512i b, int imm);

VPRORQ __m512i _mm512_maskz_ror_epi64(__mmask8 k, __m512i a, int imm);

VPRORQ __m256i _mm256_ror_epi64(__m256i a, int imm);

VPRORQ __m256i _mm256_mask_ror_epi64(__m256i a, __mmask8 k, __m256i b, int imm);

VPRORQ __m256i _mm256_maskz_ror_epi64( __mmask8 k, __m256i a, int imm);

VPRORQ __m128i _mm_ror_epi64(__m128i a, int imm);

VPRORQ __m128i _mm_mask_ror_epi64(__m128i a, __mmask8 k, __m128i b, int imm);

VPRORQ __m128i _mm_maskz_ror_epi64( __mmask8 k, __m128i a, int imm);

VPRORVD __m512i _mm512_rorv_epi32(__m512i a, __m512i cnt);

VPRORVD __m512i _mm512_mask_rorv_epi32(__m512i a, __mmask16 k, __m512i b, __m512i cnt);

VPRORVD __m512i _mm512_maskz_rorv_epi32(__mmask16 k, __m512i a, __m512i cnt);

VPRORVD __m256i _mm256_rorv_epi32(__m256i a, __m256i cnt);

VPRORVD __m256i _mm256_mask_rorv_epi32(__m256i a, __mmask8 k, __m256i b, __m256i cnt);

VPRORVD __m256i _mm256_maskz_rorv_epi32(__mmask8 k, __m256i a, __m256i cnt);

VPRORVD __m128i _mm_rorv_epi32(__m128i a, __m128i cnt);

VPRORVD __m128i _mm_mask_rorv_epi32(__m128i a, __mmask8 k, __m128i b, __m128i cnt);

VPRORVD __m128i _mm_maskz_rorv_epi32(__mmask8 k, __m128i a, __m128i cnt);

VPRORVQ __m512i _mm512_rorv_epi64(__m512i a, __m512i cnt);

VPRORVQ __m512i _mm512_mask_rorv_epi64(__m512i a, __mmask8 k, __m512i b, __m512i cnt);

VPRORVQ __m512i _mm512_maskz_rorv_epi64( __mmask8 k, __m512i a, __m512i cnt);

VPRORVQ __m256i _mm256_rorv_epi64(__m256i a, __m256i cnt);

VPRORVQ __m256i _mm256_mask_rorv_epi64(__m256i a, __mmask8 k, __m256i b, __m256i cnt);

VPRORVQ __m256i _mm256_maskz_rorv_epi64(__mmask8 k, __m256i a, __m256i cnt);

VPRORVQ __m128i _mm_rorv_epi64(__m128i a, __m128i cnt);

VPRORVQ __m128i _mm_mask_rorv_epi64(__m128i a, __mmask8 k, __m128i b, __m128i cnt);

VPRORVQ __m128i _mm_maskz_rorv_epi64(__mmask8 k, __m128i a, __m128i cnt);
.EE

.SS SIMD FLOATING-POINT EXCEPTIONS <a
href="./vprord:vprorvd:vprorq:vprorvq.html#simd-floating-point-exceptions"
class="anchor">¶

.PP
None.

.SS OTHER EXCEPTIONS  href="./vprord:vprorvd:vprorq:vprorvq.html#other-exceptions"
class="anchor">¶

.PP
EVEX-encoded instruction, see Table
2-49, “Type E4 Class Exception Conditions.”

.SH COLOPHON
This UNOFFICIAL, mechanically-separated, non-verified reference is
provided for convenience, but it may be
incomplete or
broken in various obvious or non-obvious ways.
Refer to Intel® 64 and IA-32 Architectures Software Developer’s
Manual
\[la]https://software.intel.com/en\-us/download/intel\-64\-and\-ia\-32\-architectures\-sdm\-combined\-volumes\-1\-2a\-2b\-2c\-2d\-3a\-3b\-3c\-3d\-and\-4\[ra]
for anything serious.

.br
This page is generated by scripts; therefore may contain visual or semantical bugs. Please report them (or better, fix them) on https://github.com/MrQubo/x86-manpages.
