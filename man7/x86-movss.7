'\" t
.nh
.TH "X86-MOVSS" "7" "December 2023" "Intel" "Intel x86-64 ISA Manual"
.SH NAME
MOVSS - MOVE OR MERGE SCALAR SINGLE PRECISION FLOATING-POINT VALUE
.TS
allbox;
l l l l l 
l l l l l .
\fBOpcode/Instruction\fP	\fBOp / En\fP	\fB64/32 bit Mode Support\fP	\fBCPUID Feature Flag\fP	\fBDescription\fP
F3 0F 10 /r MOVSS xmm1, xmm2	A	V/V	SSE	T{
Merge scalar single precision floating-point value from xmm2 to xmm1 register.
T}
F3 0F 10 /r MOVSS xmm1, m32	A	V/V	SSE	T{
Load scalar single precision floating-point value from m32 to xmm1 register.
T}
T{
VEX.LIG.F3.0F.WIG 10 /r VMOVSS xmm1, xmm2, xmm3
T}	B	V/V	AVX	T{
Merge scalar single precision floating-point value from xmm2 and xmm3 to xmm1 register
T}
T{
VEX.LIG.F3.0F.WIG 10 /r VMOVSS xmm1, m32
T}	D	V/V	AVX	T{
Load scalar single precision floating-point value from m32 to xmm1 register.
T}
T{
F3 0F 11 /r MOVSS xmm2/m32, xmm1
T}	C	V/V	SSE	T{
Move scalar single precision floating-point value from xmm1 register to xmm2/m32.
T}
T{
VEX.LIG.F3.0F.WIG 11 /r VMOVSS xmm1, xmm2, xmm3
T}	E	V/V	AVX	T{
Move scalar single precision floating-point value from xmm2 and xmm3 to xmm1 register.
T}
T{
VEX.LIG.F3.0F.WIG 11 /r VMOVSS m32, xmm1
T}	C	V/V	AVX	T{
Move scalar single precision floating-point value from xmm1 register to m32.
T}
T{
EVEX.LLIG.F3.0F.W0 10 /r VMOVSS xmm1 {k1}{z}, xmm2, xmm3
T}	B	V/V	AVX512F	T{
Move scalar single precision floating-point value from xmm2 and xmm3 to xmm1 register under writemask k1.
T}
T{
EVEX.LLIG.F3.0F.W0 10 /r VMOVSS xmm1 {k1}{z}, m32
T}	F	V/V	AVX512F	T{
Move scalar single precision floating-point values from m32 to xmm1 under writemask k1.
T}
T{
EVEX.LLIG.F3.0F.W0 11 /r VMOVSS xmm1 {k1}{z}, xmm2, xmm3
T}	E	V/V	AVX512F	T{
Move scalar single precision floating-point value from xmm2 and xmm3 to xmm1 register under writemask k1.
T}
T{
EVEX.LLIG.F3.0F.W0 11 /r VMOVSS m32 {k1}, xmm1
T}	G	V/V	AVX512F	T{
Move scalar single precision floating-point values from xmm1 to m32 under writemask k1.
T}
.TE

.SH INSTRUCTION OPERAND ENCODING
.TS
allbox;
l l l l l l 
l l l l l l .
\fBOp/En\fP	\fBTuple Type\fP	\fBOperand 1\fP	\fBOperand 2\fP	\fBOperand 3\fP	\fBOperand 4\fP
A	N/A	ModRM:reg (r, w)	ModRM:r/m (r)	N/A	N/A
B	N/A	ModRM:reg (w)	VEX.vvvv (r)	ModRM:r/m (r)	N/A
C	N/A	ModRM:r/m (w)	ModRM:reg (r)	N/A	N/A
D	N/A	ModRM:reg (w)	ModRM:r/m (r)	N/A	N/A
E	N/A	ModRM:r/m (w)	EVEX.vvvv (r)	ModRM:reg (r)	N/A
F	Tuple1 Scalar	ModRM:reg (r, w)	ModRM:r/m (r)	N/A	N/A
G	Tuple1 Scalar	ModRM:r/m (w)	ModRM:reg (r)	N/A	N/A
.TE

.SH DESCRIPTION
Moves a scalar single precision floating-point value from the source
operand (second operand) to the destination operand (first operand). The
source and destination operands can be XMM registers or 32-bit memory
locations. This instruction can be used to move a single precision
floating-point value to and from the low doubleword of an XMM register
and a 32-bit memory location, or to move a single precision
floating-point value between the low doublewords of two XMM registers.
The instruction cannot be used to transfer data between memory
locations.

.PP
Legacy version: When the source and destination operands are XMM
registers, bits (MAXVL-1:32) of the corresponding destination register
are unmodified. When the source operand is a memory location and
destination

.PP
operand is an XMM registers, Bits (127:32) of the destination operand is
cleared to all 0s, bits MAXVL:128 of the destination operand remains
unchanged.

.PP
VEX and EVEX encoded register-register syntax: Moves a scalar single
precision floating-point value from the second source operand (the third
operand) to the low doubleword element of the destination operand (the
first operand). Bits 127:32 of the destination operand are copied from
the first source operand (the second operand). Bits (MAXVL-1:128) of the
corresponding destination register are zeroed.

.PP
VEX and EVEX encoded memory load syntax: When the source operand is a
memory location and destination operand is an XMM registers, bits
MAXVL:32 of the destination operand is cleared to all 0s.

.PP
EVEX encoded versions: The low doubleword of the destination is updated
according to the writemask.

.PP
Note: For memory store form instruction “VMOVSS m32, xmm1”, VEX.vvvv is
reserved and must be 1111b otherwise instruction will #UD. For memory
store form instruction “VMOVSS mv {k1}, xmm1”, EVEX.vvvv is reserved and
must be 1111b otherwise instruction will #UD.

.PP
Software should ensure VMOVSS is encoded with VEX.L=0. Encoding VMOVSS
with VEX.L=1 may encounter unpredictable behavior across different
processor generations.

.SH OPERATION
.SS VMOVSS (EVEX.LLIG.F3.0F.W0 11 /R WHEN THE SOURCE OPERAND IS MEMORY AND THE DESTINATION IS AN XMM REGISTER) <a
href="movss.html#vmovss--evex-llig-f3-0f-w0-11--r-when-the-source-operand-is-memory-and-the-destination-is-an-xmm-register-"
class="anchor">¶

.EX
IF k1[0] or *no writemask*
    THEN DEST[31:0] := SRC[31:0]
    ELSE
        IF *merging-masking* ; merging-masking
            THEN *DEST[31:0] remains unchanged*
            ELSE ; zeroing-masking
                THEN DEST[31:0] := 0
        FI;
FI;
DEST[MAXVL-1:32] := 0
.EE

.SS VMOVSS (EVEX.LLIG.F3.0F.W0 10 /R WHEN THE SOURCE OPERAND IS AN XMM REGISTER AND THE DESTINATION IS MEMORY) <a
href="movss.html#vmovss--evex-llig-f3-0f-w0-10--r-when-the-source-operand-is-an-xmm-register-and-the-destination-is-memory-"
class="anchor">¶

.EX
IF k1[0] or *no writemask*
    THEN DEST[31:0] := SRC[31:0]
    ELSE *DEST[31:0] remains unchanged* ; merging-masking
FI;
.EE

.SS VMOVSS (EVEX.LLIG.F3.0F.W0 10/11 /R WHERE THE SOURCE AND DESTINATION ARE XMM REGISTERS) <a
href="movss.html#vmovss--evex-llig-f3-0f-w0-10-11--r-where-the-source-and-destination-are-xmm-registers-"
class="anchor">¶

.EX
IF k1[0] or *no writemask*
    THEN DEST[31:0] := SRC2[31:0]
    ELSE
        IF *merging-masking* ; merging-masking
            THEN *DEST[31:0] remains unchanged*
            ELSE ; zeroing-masking
                THEN DEST[31:0] := 0
        FI;
FI;
DEST[127:32] := SRC1[127:32]
DEST[MAXVL-1:128] := 0
.EE

.SS MOVSS (LEGACY SSE VERSION WHEN THE SOURCE AND DESTINATION OPERANDS ARE BOTH XMM REGISTERS) <a
href="movss.html#movss--legacy-sse-version-when-the-source-and-destination-operands-are-both-xmm-registers-"
class="anchor">¶

.EX
DEST[31:0] := SRC[31:0]
DEST[MAXVL-1:32] (Unmodified)
.EE

.SS VMOVSS (VEX.128.F3.0F 11 /R WHERE THE DESTINATION IS AN XMM REGISTER) <a
href="movss.html#vmovss--vex-128-f3-0f-11--r-where-the-destination-is-an-xmm-register-"
class="anchor">¶

.EX
DEST[31:0] := SRC2[31:0]
DEST[127:32] := SRC1[127:32]
DEST[MAXVL-1:128] := 0
.EE

.SS VMOVSS (VEX.128.F3.0F 10 /R WHERE THE SOURCE AND DESTINATION ARE XMM REGISTERS) <a
href="movss.html#vmovss--vex-128-f3-0f-10--r-where-the-source-and-destination-are-xmm-registers-"
class="anchor">¶

.EX
DEST[31:0] := SRC2[31:0]
DEST[127:32] := SRC1[127:32]
DEST[MAXVL-1:128] := 0
.EE

.SS VMOVSS (VEX.128.F3.0F 10 /R WHEN THE SOURCE OPERAND IS MEMORY AND THE DESTINATION IS AN XMM REGISTER) <a
href="movss.html#vmovss--vex-128-f3-0f-10--r-when-the-source-operand-is-memory-and-the-destination-is-an-xmm-register-"
class="anchor">¶

.EX
DEST[31:0] := SRC[31:0]
DEST[MAXVL-1:32] := 0
.EE

.SS MOVSS/VMOVSS (WHEN THE SOURCE OPERAND IS AN XMM REGISTER AND THE DESTINATION IS MEMORY) <a
href="movss.html#movss-vmovss--when-the-source-operand-is-an-xmm-register-and-the-destination-is-memory-"
class="anchor">¶

.EX
DEST[31:0] := SRC[31:0]
.EE

.SS MOVSS (LEGACY SSE VERSION WHEN THE SOURCE OPERAND IS MEMORY AND THE DESTINATION IS AN XMM REGISTER) <a
href="movss.html#movss--legacy-sse-version-when-the-source-operand-is-memory-and-the-destination-is-an-xmm-register-"
class="anchor">¶

.EX
DEST[31:0] := SRC[31:0]
DEST[127:32] := 0
DEST[MAXVL-1:128] (Unmodified)
.EE

.SH INTEL C/C++ COMPILER INTRINSIC EQUIVALENT  href="movss.html#intel-c-c++-compiler-intrinsic-equivalent"
class="anchor">¶

.EX
VMOVSS __m128 _mm_mask_load_ss(__m128 s, __mmask8 k, float * p);

VMOVSS __m128 _mm_maskz_load_ss( __mmask8 k, float * p);

VMOVSS __m128 _mm_mask_move_ss(__m128 sh, __mmask8 k, __m128 sl, __m128 a);

VMOVSS __m128 _mm_maskz_move_ss( __mmask8 k, __m128 s, __m128 a);

VMOVSS void _mm_mask_store_ss(float * p, __mmask8 k, __m128 a);

MOVSS __m128 _mm_load_ss(float * p)

MOVSS void_mm_store_ss(float * p, __m128 a)

MOVSS __m128 _mm_move_ss(__m128 a, __m128 b)
.EE

.SH SIMD FLOATING-POINT EXCEPTIONS
None.

.SH OTHER EXCEPTIONS
Non-EVEX-encoded instruction, see Table
2-22, “Type 5 Class Exception Conditions,” additionally:

.TS
allbox;
l l 
l l .
\fB\fP	\fB\fP
#UD	If VEX.vvvv != 1111B.
.TE

.PP
EVEX-encoded instruction, see Table
2-58, “Type E10 Class Exception Conditions.”

.SH COLOPHON
This UNOFFICIAL, mechanically-separated, non-verified reference is
provided for convenience, but it may be
incomplete or
broken in various obvious or non-obvious ways.
Refer to Intel® 64 and IA-32 Architectures Software Developer’s
Manual
\[la]https://software.intel.com/en\-us/download/intel\-64\-and\-ia\-32\-architectures\-sdm\-combined\-volumes\-1\-2a\-2b\-2c\-2d\-3a\-3b\-3c\-3d\-and\-4\[ra]
for anything serious.

.br
This page is generated by scripts; therefore may contain visual or semantical bugs. Please report them (or better, fix them) on https://github.com/MrQubo/x86-manpages.
